# ğŸ“¦ Repository Name
**VAE-Phone-Object-Generator**

# ğŸ“ Repository Description
A lightweight Variational Autoencoder (VAE) project trained on smartphone-captured images to generate synthetic samples, compare **BCE vs MSE** reconstruction losses, visualize training behavior, and demonstrate KL divergence in meanâ€“variance form. Focused on visuals with minimal text.

---

# ğŸ“· Sample Dataset
<img src="sample_dataset.png" width="500"/>

---

# ğŸ”§ VAE Reconstructions (BCE vs MSE)
<img src="vae_recon_images_with_bce_mse.png" width="850"/>

---

# ğŸ“‰ Training Curves

### âœ… Total Loss  
<img src="total_loss_bce_vs_mse.png" width="700"/>

### âœ… Reconstruction Loss  
<img src="recon_loss_bce_vs_mse.png" width="700"/>

### âœ… KL Divergence  
<img src="kl_loss_bce_vs_mse.png" width="700"/>

---

# ğŸ§ª Generated Images From Prior Distribution

### ğŸ¯ MSE-Trained Model  
<img src="generated_from_prior_mse.png" width="850"/>

### ğŸ¯ BCE-Trained Model  
<img src="generated_from_prior_BCE.png" width="850"/>

---

# ğŸ§® KL Divergence (Mean & Variance Form)

\[
D_{KL}(q||p)
= -\frac{1}{2}\sum_{i=1}^{d}
\left(1 + \log\sigma_i^2 - \mu_i^2 - \sigma_i^2\right)
\]

---

# âœ”ï¸ Quick Summary
- **BCE** â†’ Sharper, higher-scale loss  
- **MSE** â†’ Smoother, higher PSNR & SSIM  
- VAE successfully reconstructs + generates new samples  
- KL term regularizes latent distribution  

---
